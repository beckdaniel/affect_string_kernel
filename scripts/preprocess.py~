import numpy as np
import nltk
from nltk.tokenize import wordpunct_tokenize


def preprocess_sent(sent, lemmatizer):
    """
    Take a sentence in string format and returns
    a list of lemmatized tokens.
    """
    tokenized = wordpunct_tokenize(sent.lower())
    sent = [lemmatizer.lemmatize(word) for word in tokenized]
    return sent
